{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SKETCHBOOK \n",
    "#### For the sparsity penalty implementation\n",
    "\n",
    "Different implementation using\n",
    "1. a class for the activity regulariser based on Keras Regularizer class (*attribute problem* so far)\n",
    "2. a function to apply that takes as input the activation layer (but *how to get the activation class?*)\n",
    "3. a *splitted* model that gives an intermediate output to compute the mean over and wraps the function defined above inside a lambda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras import regularizers\n",
    "\n",
    "# using leaky relu?\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "# using mnist\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _),(x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "\n",
    "#x_train = np.random.rand(128,128,100).astype('float32')\n",
    "#x_train = np.reshape(a=x_train,newshape=[100,128,128,1])\n",
    "#p = K.constant(value=5e-2,shape=(1,1)) #kullback is expecting two tensors of the same shape\n",
    "#print(kl.shape, beta.shape, p.shape)\n",
    "#print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compile it only for splitted model\n",
    "def penaltyTerm(yTrue,yPred):\n",
    "    return beta*(losses.kullback_leibler_divergence(p, yPred)/100) # hard coded since predefined batch size\n",
    "\n",
    "def customLoss(yTrue,yPred):\n",
    "    return losses.mean_squared_error(yTrue,yPred) + penaltyTerm(yTrue,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# OPTION 1\n",
    "# the KL divergence describe the penalty term to be applied to the loss function\n",
    "def KL(p, p_hat):\n",
    "    return (p * K.log(p / p_hat)) + ((1-p) * K.log((1-p) / (1-p_hat)))\n",
    "\n",
    "class SparseReg(regularizers.Regularizer):\n",
    "\n",
    "    def __init__(self, p=0.05, beta=0.1,p_hat=0.0):\n",
    "        self.p = K.cast_to_floatx(p)\n",
    "        self.beta = K.cast_to_floatx(beta)\n",
    "        self.p_hat = K.cast_to_floatx(p_hat)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        regularization = 0.\n",
    "        # p_hat needs to be the average activation of the units in the hidden layer.      \n",
    "        self.p_hat = K.sum(K.mean(x))\n",
    "\n",
    "        regularization += self.beta * KL(self.p,self.p_hat)\n",
    "        return regularization\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'p': float(self.p),\n",
    "                'beta': float(self.beta)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# OPTION 2\n",
    "#define a custom sparse loss\n",
    "# the KL divergence describe the penalty term to be applied to the loss function\n",
    "def KL(p, p_hat):\n",
    "    return (p * K.log(p / p_hat)) + ((1-p) * K.log((1-p) / (1-p_hat)))\n",
    "\n",
    "# define a custom activity regularisation function\n",
    "# obs: the function can be wrapped inside a lambda layer\n",
    "def sparse_reg(x):\n",
    "    p = 0.05; # desired average activation of the hidden units\n",
    "    beta = 0.1; # weight of sparsity penalty term\n",
    "    # axis 0 batch_size, axis 1 layer size\n",
    "    p_hat = K.mean(x, axis=0) # average over the batch samples\n",
    "    return KL(p, p_hat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# encoder\n",
    "inp = Input(shape=(28,28,1))\n",
    "lay = Conv2D(filters=16,kernel_size=(4,4),padding='same',\n",
    "             activation=PReLU(),activity_regularizer=SparseReg(beta=5e-1,p=1e-2),name='encoder')(inp)\n",
    "\n",
    "# computes on the top of the hidden layer\n",
    "# obs: a Lambda layer is used to evaluate the sparse regularisation function\n",
    "#layMean = Lambda(lambda x: sparse_reg(x),name='layMean')(lay)\n",
    "#laySum = Lambda(lambda x: K.sum(x),name='laySum')(layMean)\n",
    "layMean = Lambda(lambda x: K.mean(x),name='layMean')(lay)\n",
    "\n",
    "# decoder\n",
    "out = Conv2D(filters=1,kernel_size=(4,4),padding='same',activation=LeakyReLU(0.03) ,name='decoder')(lay)\n",
    "#outMean = Lambda(lambda x: sparse_reg(x),output_shape=(1,1))(out)\n",
    "#outSum = Lambda(lambda x: K.sum(x))(outMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a model that uses the custom loss function\n",
    "# obs: to use a specific output, the model should be splitted for that output\n",
    "model = Model(inputs=inp,outputs=out,name='sparse_cae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define a custom loss function\n",
    "def customLoss(yTrue,yPred):\n",
    "    return losses.mean_squared_error(yTrue,yPred) + K.sum(losses.kullback_leibler_divergence(p,layMean))\n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(x_train, x_train, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "# using prelu?\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "    \n",
    "# Model definition\n",
    "# encoder\n",
    "inp = Input(shape=(16,))\n",
    "lay = Dense(64, kernel_initializer='uniform',activation=PReLU(), name='encoder')(inp)\n",
    "#decoder\n",
    "out = Dense(2,kernel_initializer='uniform',activation=PReLU(), name='decoder')(lay)\n",
    "\n",
    "# build the model\n",
    "model = Model(inputs=inp,outputs=out,name='cae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
